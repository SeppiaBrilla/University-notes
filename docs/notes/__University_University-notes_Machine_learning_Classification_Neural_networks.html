<!DOCTYPE <!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
        <script src="../js/functions.js"></script>
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <link href="../css/style.css" rel="stylesheet">
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.5.1/mermaid.min.js" integrity="sha512-gAfYc+bjXAmXUHDG1Dm8AiUWDz8PhByz2852OW/ZitnuM4gGZPD8oQhz57KR2WcDcoCvZInSH1HqbZwMpjHdeg==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.3.0/css/all.min.css">
        <title>Neural networks</title>
    </head>
    <body class="dark-theme-bg">
        <div id="Content" class="content">
            <header id="TitleDiv" class="dark-theme-color">
                <h1>Neural networks</h1>
            </header>
            <nav id="OptionDiv">
                <a href="../index.html"><i class="fas fa-home icon dark-theme-color"></i></a>
                <button onclick="BackFn()"><i class="fas fa-arrow-left icon dark-theme-color"></i></button>
                <button onclick="ThemeFn()"><i class="fas fa-adjust icon dark-theme-color"></i></button>
            </nav>
            <div class="main dark-theme-color">
                <p>A neural network arranges many <a href="./__University_University-notes_Machine_learning_Classification_Perceptron.html">Perceptron</a>
 in a hierarchical structure.
Each node / layer pass a wighted signal to the next one(s).
This method allow surpassing the linearity of the <a href="./__University_University-notes_Machine_learning_Classification_Perceptron.html">Perceptron</a>
.
Then there is a system-wide trashold that is modeled as a matematical function. This function must be continuos and differetiable, superiorly and inferiorly limited in order to simplify the mathematics (sigmoid, arctangent, ...).</p>
<h2>Feed-forward multi-layered checking</h2>
<p>There are three layers: Input layer, hidden layer and output layer.</p>
<h3>Input layer</h3>
<p>it has a node for each dimension in the training set. It feed (with weight) the</p>
<h3>Hidden layer</h3>
<p>The number of nodes in this layer is a parameter of the network and, after the computation, they feed (with weights) the</p>
<h3>Output layer</h3>
<p>It has one node for each class or, in the case of 2 classes, one node only.</p>
<h2>Example</h2>
<p><img src="./__University_University-notes_imgs_Pasted_image_20221226125042.png" alt="Pasted image 20221226125042.png">

The function g() is the transfer function fo the node (like the sigmoid)
An input <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mn>0</mn></msub><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">x_0 = 1</annotation></semantics></math></span><span class="katex-html ObsidianMath" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1</span></span></span></span> is added to the network in order to deal with bias like in the case of <a href="./__University_University-notes_Machine_learning_Classification_Perceptron.html">Perceptron</a>
. The weights connect two nodes for each edge. A node can pass informations only to the following layer and all the nodes in a leyer are connected to all the nodes of the following one.</p>
<h3>Pseudocode</h3>
<pre><code class="language-python">def Neural_network()
w = [random() for d in D] 
while not terminating_conditions:
    for(x in training_instances):
        nn_result = train(neural_network, x, w)
        w = adjust_weights(w,nn_result)
        propagate_back(w)
</code></pre>
<p>At the end, we will have a set of weights that are not easly understandable and that encodes the knowledge of the network on the training set.
The convergence is not guaranteed.
There are also a set of important issues to address:</p>
<ul>
<li>computing the wheight correction</li>
<li>preparation of the train set to have 0 mean and unit variance</li>
<li>set the terminating conditions</li>
</ul>
<h2>Compute the error</h2>
<p>Let x be the input and y be the correct class of x. With w as the weights we can compute the error as <a href="./__University_University-notes_Statistical_and_Mathematical_Methods_for_Artificial_Intelligence_Linear_system_solution_linear_least-squares_problem.html">linear least-squares problem</a>
: 
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>E</mi><mo stretchy="false">(</mo><mi>w</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mo stretchy="false">(</mo><mi>y</mi><mo>−</mo><mtext>Transfer</mtext><mo stretchy="false">(</mo><mi>w</mi><mo separator="true">,</mo><mi>x</mi><mo stretchy="false">)</mo><msup><mo stretchy="false">)</mo><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">
E(w) = \frac{1}{2} (y - \text{Transfer}(w,x))^2
</annotation></semantics></math></span><span class="katex-html ObsidianMath" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.0074em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3214em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">2</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.1141em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">Transfer</span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span></p>
<p>The computation, then, looks a lot like the <a href="./__University_University-notes_Statistical_and_Mathematical_Methods_for_Artificial_Intelligence_optimization_Gradient_method.html">Gradient method</a>
 or the <a href="./__University_University-notes_Statistical_and_Mathematical_Methods_for_Artificial_Intelligence_optimization_Stochastic_gradient_descent_(SGD).html">Stochastic gradient descent (SGD)</a>
.</p>
<p>==&gt;</p>
<h3>revisited pseudocode</h3>
<pre><code class="language-python">def Neural_network()
w = [random() for d in D] 
while not terminating_conditions:
    for(x in training_instances):
        nn_result = train(neural_network, x, w)
        error = error_compute(nn_result)
        derivate = derivate(x,w)
        w = adjust_weights(w,derivate) #both for hidden and output layers
        propagate_back(w)
</code></pre>
<h2>Learning models</h2>
<p>We can use different learning models to update the weights of the network:</p>
<ul>
<li>Stocastic: it updates the weights right after the propagation. This can introduce some noise in the gradient computation but reduces the chance of getting stucked in a local minima</li>
<li>Batch: It updates the weights after many propagations and it is generally faster since it moves in the direction of the avarage error computed on the propagations</li>
</ul>
<h2>Ripetitions</h2>
<p>A learning over the whole samples of the network is called <strong>epoch</strong>. After each epoch the classification capability will be improved. Notes on <a href="./__University_University-notes_Statistical_and_Mathematical_Methods_for_Artificial_Intelligence_optimization_Stochastic_gradient_descent_(SGD).html">Stochastic gradient descent (SGD)</a>
 that uses a similiar structure.</p>
<h2>Stopping criteria</h2>
<p>We can have a set of stopping criteria that may change for each problem but, the most common one are:</p>
<ul>
<li>&quot;Small&quot; weight update in an epoch</li>
<li>The error rate goes below a certain predefined target</li>
<li>A timeout condition is reached (maximum cycle, maximum cpu usage, ...)</li>
</ul>

            </div>
        </div>
    </body>
</html>
