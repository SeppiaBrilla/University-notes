<!DOCTYPE <!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
        <script src="../js/functions.js"></script>
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <link href="../css/style.css" rel="stylesheet">
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.5.1/mermaid.min.js" integrity="sha512-gAfYc+bjXAmXUHDG1Dm8AiUWDz8PhByz2852OW/ZitnuM4gGZPD8oQhz57KR2WcDcoCvZInSH1HqbZwMpjHdeg==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.3.0/css/all.min.css">
        <title>Transformers</title>
    </head>
    <body class="dark-theme-bg">
        <div id="Content" class="content">
            <header id="TitleDiv" class="dark-theme-color">
                <h1>Transformers</h1>
            </header>
            <nav id="OptionDiv">
                <a href="../index.html"><i class="fas fa-home icon dark-theme-color"></i></a>
                <button onclick="BackFn()"><i class="fas fa-arrow-left icon dark-theme-color"></i></button>
                <button onclick="ThemeFn()"><i class="fas fa-adjust icon dark-theme-color"></i></button>
            </nav>
            <div class="main dark-theme-color">
                <p>A transformer has a traditional encoder-decoder structure, with connections between them.
The encoding component is a stack of encoders. Similarly, the decoding component is a stack of decoders.
<img src="./imgs_Pasted_image_20230520185605.png" alt="Pasted image 20230520185605.png">

<img src="./imgs_Pasted_image_20230520185621.png" alt="Pasted image 20230520185621.png">

The encoder is organized as a self-<a href="./NLP_Attention_(AI).html">Attention (AI)</a>
 layer (query, key and value are shared), followed by feedfoward component (a couple of dense layers). The decoder is similar, with an additional <a href="./NLP_Attention_(AI).html">Attention (AI)</a>
 layer that helps the decoder to focus on relevant parts of the input sentence. Using multiple heads for attention expands the model’s ability<br>to focus on different positions, for different purposes. As a result, multiple “representation subspaces” are created, focusing on potentially different aspects of the input sequence.</p>

            </div>
        </div>
    </body>
</html>
