<!DOCTYPE <!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
        <script src="../js/functions.js"></script>
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <link href="../css/style.css" rel="stylesheet">
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.5.1/mermaid.min.js" integrity="sha512-gAfYc+bjXAmXUHDG1Dm8AiUWDz8PhByz2852OW/ZitnuM4gGZPD8oQhz57KR2WcDcoCvZInSH1HqbZwMpjHdeg==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.3.0/css/all.min.css">
        <title>Machine learning procedures and examples</title>
    </head>
    <body class="dark-theme-bg">
        <div id="Content" class="content">
            <header id="TitleDiv" class="dark-theme-color">
                <h1>Machine learning procedures and examples</h1>
            </header>
            <nav id="OptionDiv">
                <a href="../index.html"><i class="fas fa-home icon dark-theme-color"></i></a>
                <button onclick="BackFn()"><i class="fas fa-arrow-left icon dark-theme-color"></i></button>
                <button onclick="ThemeFn()"><i class="fas fa-adjust icon dark-theme-color"></i></button>
            </nav>
            <div class="main dark-theme-color">
                <h1><a href="./Machine_learning_Classification_Classification.html">Classification</a>
</h1>
<h2><a href="./Machine_learning_Classification_Decision_tree.html">Decision tree</a>
</h2>
<pre><code class="language-python">from sklearn.tree import DecisionTreeClassifier
</code></pre>
<p>The only parameter is &#39;max_depth&#39; and it can vary from 1 to decision_tree.tree_.max_depth with decision_tree the tree obtained with unlimited max depth.</p>
<h2><a href="./Machine_learning_Classification_K_nearest_classifier.html">K nearest classifier</a>
</h2>
<pre><code class="language-python">from sklearn.neighbors import KNeighborsClassifier
</code></pre>
<p>The only parameter is n_neighbors and a good start for the guess is the sqrt of the number of datapoints.</p>
<h2><a href="./Machine_learning_Classification_Adaboost.html">Adaboost</a>
</h2>
<pre><code class="language-python">from sklearn.ensemble import AdaBoostClassifier

params = {&#39;n_estimators&#39;:[20,30,40,50], &#39;learning_rate&#39;:[0.5,0.75,1,1.25,1.5]}
</code></pre>
<h1><a href="./Machine_learning_Clustering_Clustering.html">Clustering</a>
</h1>
<h2><a href="./Machine_learning_Clustering_K-means.html">K-means</a>
</h2>
<pre><code class="language-python">from sklearn.cluster import KMeans
</code></pre>
<p>It only needs the number of clusters to search. The inertia can be obtained with k_means.inertia_. </p>
<h2><a href="./Machine_learning_Clustering_Density_based_clustering.html">DBSCAN</a>
</h2>
<pre><code class="language-python">from sklearn.cluster import DBSCAN

dbscan = DBSCAN()
y_pred = dbscan.fit_predict(X)

n_clusters = np.unique(y_pred[y_pred != -1])
if n_clusters &gt; 1:
    silhouette_value = silhouette_score(X, y_pred)
</code></pre>
<p>The parameters are eps and min_sample. min_sample can be 2 * dimensions of the dataset. eps can be chosen as the elbow of the k-distance with k = min_sample </p>
<h2>Gaussian mixture (<a href="./Machine_learning_Clustering_Model_based_clustering.html">Model based clustering</a>
)</h2>
<pre><code class="language-python">from sklearn.mixture import GaussianMixture
params = [{&#39;n_components&#39;:range(2,4), &#39;max_iter&#39;:range(100,500,10), &#39;n_init&#39;:range(1,4), &#39;init_params&#39;:[&#39;kmeans&#39;, &#39;k-means++&#39;, &#39;random&#39;]}]
</code></pre>
<h2>Agglomerative (<a href="./Machine_learning_Clustering_Hierarchical_clustering.html">Hierarchical clustering</a>
)</h2>
<pre><code class="language-python">from sklearn.cluster import AgglomerativeClustering
params = [{&#39;n_clusters&#39;:range(2,4), &#39;metric&#39;:[&#39;euclidean&#39;, &#39;l1&#39;, &#39;l2&#39;, &#39;manhattan&#39;, &#39;cosine&#39;], &#39;linkage&#39;:[&#39;ward&#39;, &#39;complete&#39;, &#39;average&#39;, &#39;single&#39;]}]
if parameter[&#39;linkage&#39;] == &#39;ward&#39; and parameter[&#39;metric&#39;] != &#39;euclidean&#39;:
    continue
</code></pre>
<h3>K distance</h3>
<pre><code class="language-python">def k_distance(X, k):

    k_distances = []
    for i in range(0, X.shape[0]):
        k_point_distances = []
        for j in range(0, X.shape[0]):
            if i!=j:
                dist = np.sqrt(sum((X[i,:]-X[j,:])**2))
                k_point_distances.append(dist)
        k_point_distances.sort()
        k_distances.append(k_point_distances[k-1])
        
    k_distances.sort(reverse=True)
    return k_distances
</code></pre>
<h1>Extra</h1>
<pre><code class="language-python">from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import ParameterGrid
from mlxtend.plotting import plot_confusion_matrix
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import cross_val_score
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import OrdinalEncoder
from sklearn.metrics.cluster import silhouette_score

import warnings
warnings.filterwarnings(&#39;ignore&#39;)

try:
    import google.colab.files
    IN_COLAB = True
except:
    IN_COLAB = False
    # from google.colab import files
if IN_COLAB:
    uploaded = files.upload()

data = data[~data.isnull().any(axis=1)] # remove rows with null values 
data = data.dropna(how=&#39;any&#39;)
data.isnull().sum() &gt; 0 # check column with null values

cross_val_score(model, X_train, y_train, scoring=&#39;accuracy&#39;)

plot_confusion_matrix(confusion_matrix_test, show_normed=True);

def two_plots(x, y1, y2, xlabel, y1label, y2label):

    fig, ax1 = plt.subplots()
    
    color = &#39;tab:red&#39;
    ax1.set_xlabel(xlabel)
    ax1.set_ylabel(y1label, color=color)
    ax1.plot(x, y1, color=color)
    ax1.tick_params(axis=&#39;y&#39;, labelcolor=color)
    
    ax2 = ax1.twinx() # instantiate a second axes that shares the same x-axis
    color = &#39;tab:blue&#39;
    ax2.set_ylabel(y2label, color=color) # we already handled the x-label with ax1
    ax2.plot(x, y2, color=color)
    ax2.tick_params(axis=&#39;y&#39;, labelcolor=color)
    ax2.set_ylim(0,1) # the axis for silhouette is [0,1]
    fig.tight_layout() # otherwise the right y-label is slightly clipped
    
    plt.show()


def transform(y_true, y_pred):
    y_mapped = y_pred.copy()
    for lab in np.unique(y_pred):
        if y_true.str.contains(lab).any():
            true_l, count = np.unique(y_true[y_pred==lab], return_counts=True)
            y_mapped[y_pred==lab] = true_l[np.argmax(count)]
        else:
            y_mapped[y_pred==lab] = lab
    return y_mapped
</code></pre>
<h3>General training process</h3>
<pre><code class="language-python">paramters_range = [{&#39;p1&#39;:value_range_p1, &#39;...&#39;, &#39;pn&#39;:value_range_pn}]
parameters = list(ParameterGrid(paramters_range))

scores = []

for i in range(len(parameters)):
    model = Model(**(parameters[i]))
    model.fit(x_train, y_train) # y_train only if needed
    y_pred = model.predict(x_test)
    score = compute_Score_with_function(y_test, y_pred)
    
    scores.append(score)

best_parameters = parameters[np.argmax(scores)]
print(best_parameters)
</code></pre>

            </div>
        </div>
    </body>
</html>
