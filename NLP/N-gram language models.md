The N-gram model is a probabilistic model. Ideally, we would base the probability of the next word on all the previous ones, but the probability table would be too big to be store, therefore, we need to limit the model to a set of words. If the model is based on none previous words, is called __unigram__ while the one based on one previous word is the __bigram__. All the N-gram models are based on the [[Naive Bayes classifier]].
The easier way to estimate the probabilities of the n-gram model is using the [[Maximum likelihood estimation]] ($P(w_i|w_{i-1}) = \frac{Count(w_{i-1} w_I)}{Count(w_{i-1})}$). To avoid underflow and to use sums instead of products we work in Log space.